<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>D^2NeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://d2nerf.github.io/img/title_card.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://github.com/d2nerf/d2nerf">
    <meta property="og:title" content="D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video">
    <meta property="og:description" content="Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video">
    <meta name="twitter:description" content="Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects.">
    <meta name="twitter:image" content="https://d2nerf.github.io/img/title_card.png">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>D<sup>2</sup>NeRF</b>: Self-Supervised Decoupling of <br>Dynamic and
                Static Objects from a Monocular Video<br>
                <small>
                    NeurIPS 2022
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://chikayan.github.io/">
                                Tianhao Wu
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cl.cam.ac.uk/~fz261/">
                                Fangcheng Zhong
                            </a>
                            <br>University of Cambridge
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://taiya.github.io/">
                                Andrea Tagliasacchi
                            </a>
                            <br>Google Research
                            <br>Simon Fraser University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://people.csail.mit.edu/fcole/">
                                Forrester Cole
                            </a>
                            <br>Google Research
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cl.cam.ac.uk/~aco41/">
                                Cengiz Oztireli
                            </a>
                            <br>Google Research
                            <br>University of Cambridge
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2205.15838">
                            <img src="./img/paper_icon.png" height="90px">
                                <h4><strong>Paper (arixv)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://drive.google.com/drive/folders/1qm-8P6UqrhimZXp4USzFPumyfu8l1vto?usp=sharing">
                            <img src="./img/data_icon.webp" height="90px">
                                <h4><strong>Data</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/d2nerf/d2nerf">
                            <img src="./img/github_icon.svg" height="90px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="">
                            <img src="./img/youtube_icon.png" height="90px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>

        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="balloonDiv">
                    <video class="video" id="balloon" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop.mp4" onplay="resizeAndPlay(this)"></video>

                    <canvas height=0 class="videoMerge" id="balloonMerge"></canvas>
                </div>
			</div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- Please search on arxiv for an unanonymized version of the paper and supplementary. -->


                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water2" loop playsinline autoPlay muted src="video/vrig_vrig_water_pour.mp4" onplay="resizeAndPlayDual(this, 'water3')"></video>

                                <canvas height=0 class="videoMerge" id="water2Merge"></canvas>
                        </td>
                        <td align="left" valign="top" width="50%">
                                <video class="video" preload="auto" id="water3" loop playsinline autoPlay muted src="video/vrig_vrig_water_pour_compare_dynamic.mp4"></video>

                                <canvas height=0 class="videoMerge" id="water3Merge"></canvas>
                        </td>
                    </tr>
                </table>

            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="waterDiv">
                    <video class="video" id="water" loop playsinline autoPlay muted src="video/vrig_vrig_water_pour.mp4" onplay="resizeAndPlay(this)"></video>

                    <canvas height=0 class="videoMerge" id="waterMerge"></canvas>
                </div>
			</div>
        </div> -->

        <div class="row" height="400">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    More results
                </h3> -->

                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/vrig_vrig_duck_jump_compare_test.mp4" type="video/mp4" />
                    </video>
                </div>
                Please refresh the page if videos are not loaded or appear unsynced.
            </div>
        </div>







        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects.
                </p>
            </div>
        </div>

        <image src="img/method.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">




            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        More results
                    </h3>

                    <div class="text-justify">
                        Our method decouples and reconstructs 3D models of dynamic occluders and static background from a monocular video:
                        <br><br>

                    </div>

                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="balloon1" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop.mp4" onplay="resizeAndPlayDual(this, 'balloon2')"></video>

                                    <canvas height=0 class="videoMerge" id="balloon1Merge"></canvas>
                            </td>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="balloon2" loop playsinline autoPlay muted src="video/vrig_balloon_wave_crop_compare_dynamic.mp4"></video>

                                    <canvas height=0 class="videoMerge" id="balloon2Merge"></canvas>
                            </td>
                        </tr>
                    </table>

                </div>
            </div>

            <div class="row" height="400">
                <div class="col-md-8 col-md-offset-2">
                    <div>
                        <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                            <source src="video/vrig_balloon_wave_crop_compare_test.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">

                <div class="col-md-8 col-md-offset-2">

                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="shark1" loop playsinline autoPlay muted src="video/shark_scam.mp4" onplay="resizeAndPlayDual(this, 'shark2')"></video>

                                    <canvas height=0 class="videoMerge" id="shark1Merge"></canvas>
                            </td>
                            <td align="left" valign="top" width="50%">
                                    <video class="video" preload="auto" id="shark2" loop playsinline autoPlay muted src="video/shark_scam_compare_dynamic.mp4"></video>

                                    <canvas height=0 class="videoMerge" id="shark2Merge"></canvas>
                            </td>
                        </tr>
                    </table>

                </div>
                <!-- <div class="video-compare-container-small">
                    <video class="video" id="shark" loop playsinline autoPlay muted src="video/shark_scam.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="sharkMerge"></canvas>
                </div> -->
            </div>
        </div>




        <div class="row" height="400">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    More results
                </h3> -->

                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/pick_drop_compare_all.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-justify">
                    Our method can also remove challenging view-correlated shadows, such as shadows cast by the camera or the photographer:
                    <br><br>

                </div>
                <div class="video-compare-container" id="camShadowDiv">
                    <video class="video" id="camShadow" loop playsinline autoPlay muted src="video/camera_shadow_v2.mp4" onplay="resizeAndPlay(this)"></video>

                    <canvas height=0 class="videoMerge" id="camShadowMerge"></canvas>
                </div>
			</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-justify">
                    <br><br>
                    Our method learns decoupled neural radiance fields, we can therefore render separate components from novel view. As a by-product, we better utilize the network capacity and learn more robust dynamic objects in the scene:
                    <br><br>
                </div>
                <div>
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="padding-top: 10px;">
                        <source src="video/hypernerf_broom-vcam_compare_nv.mp4" type="video/mp4" />
                    </video>
                </div>
                <div class="video-compare-container-small" id="nvDiv">
                    <video class="video" id="nv" loop playsinline autoPlay muted src="video/hypernerf_broom-vcam_compare_nv_hn.mp4" onplay="resizeAndPlay(this)"></video>

                    <canvas height=0 class="videoMerge" id="nvMerge"></canvas>
                </div>
			</div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reflection Direction Parameterization
                </h3>
                <div class="text-justify">
                    Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results.

                    <br><br>

                </div>
                <div class="text-center">
                    <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Directional Encoding
                </h3>
                <div class="text-justify">
                    We explicitly model object roughness using the expected values of a set of spherical harmonics under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ide.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows sharing the emittance functions between points with different roughnesses. It also enables scene editing after training. Theoretically, our encoding is stationary on the sphere, similar to the Euclidean stationarity of NeRF's positional encoding. <br><br>
                </div>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Synthetic Results
                </h3>
                <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Captured Scenes
                </h3>
                Our method also produces accurate renderings and surface normals from captured photographs:
                <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div>
			</div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>

                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted>
                        <source src="video/pick_drop_compare_all.mp4" type="video/mp4" />
                    </video>
                </div>



            </div>
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>

                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">
                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                    </video>
                </div>

                <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>

                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>

            </div>
        </div> -->


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{verbin2021refnerf,
    title={{Ref-NeRF}: Structured View-Dependent Appearance for
           Neural Radiance Fields},
    author={Dor Verbin and Peter Hedman and Ben Mildenhall and
            Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
    journal={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>